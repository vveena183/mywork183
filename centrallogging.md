
Introduction
Logging and monitoring is crucial for observability of the AWS environment as well as to maintain well managed security posture. The ephemeral natural of resources means that often the only record we have of activity that happened resides in data that is captured and recorded in the form of a log.

Core tenets in designing the logging and monitoring approach:

Be proactive, log and monitor everything
Anomalous activity is detected as soon as it happens
Automation is at the basis of the logging and monitoring lifecycle.
As much as possible, detection should trigger automated correction
Requirements from E-ISO
Ensure the cloud environment includes logging & monitoring capabilities to allow for the detection of unauthorized activity.
Enable AWS CloudTrail logging
Enable VPC flow logging into CloudWatch
Enable CloudTrail S3 Bucket access logging
Required logs sent to authorized centralized log management system
Establish log metric filters and alarms in CloudWatch for the following:
Unauthorized API calls In CloudWatch
Management console logging without MFA
Usage of root accounts
CloudTrail configuration Changes
AWS Management Console authentication failures
Disabling or Deletion of Customer Created CMKs
S3 Bucket Policy Changes
AWS Config configuration changes
Security Group Changes
Changes to Network Access Control Lists (NACL)
Changes to Network Gateways
Changes to Route Tables
VPC Changes
Monitor log configuration posture
Log retention period
Ensure CloudTrail log file validation is enabled.
Logs bucket encrypted at rest
Bucket storing logs are not publicly accessible
Enable AWS Trusted Advisor for all accounts
Overview of Log Sources
AWS services provide extensive logging capabilities. Correlating this information with other event sources like operating systems, applications and databases can provide enhance visibility and help maintain robust governance posture.

Based on the scope of the log data, we can partition logging sources into three categories:

Scope	Description	Examples
Global	These log sources capture platform wide activity i.e. they are not tied to a specific resource or service.	CloudTrail, Config, Trusted Advisor
Service and resource specific	These represent log data that is generated by resources or the parent service. The scope is narrower than previous. Decision on enablement is based on business need, regulatory requirement or internal <Customer Name> policy. In general, these logs can be directly linked to an application or workload. Logs in this category are exposed by AWS in some form, commonly via S3 although not always.	S3 access logs, ALB access logs, RDS database log files etc, VPC flow logs
Native resource logs	These refer to log sources that are not handled by AWS and commonly are generated from within a specific resource, such as an application installed in an EC2 instance.	Syslog, application logs
Control Tower Centralized Logging for CloudTrail & Config
As part of the Control Tower setup, log archive account is dedicated to collecting all logs centrally, including logs for all of Control Tower managed accounts. Control Tower currently aggregates the AWS CloudTrail logs and AWS Config delivery channel (for configuration snapshots and configuration history files), to the S3 audit bucket created in the log archive account.

Aggregating Logs from Additional Sources in Control Tower Managed AWS Accounts
For aggregating logs generated by additional event sources in the Control Tower managed AWS Accounts , there are 2 alternatives:

Use the audit bucket created in the log archive account by Control Tower for storing additional logs; but for this to work, existing bucket policy of the audit bucket in the log archive bucket should be updated to allow other relevant AWS service principals to write
Create a separate bucket which is modeled similar to CT log bucket structure (i.e., bucket with object key format /AWSLogs/<account number>/<log type>/) and use this bucket to aggregate logs from additional sources in Control Tower managed AWS accounts
It was decided to use a second log aggregation S3 bucket in the Control Tower log archive account to aggregate log from additional source to avoid updating the Control Tower created bucket policy.

Centralized Logging Architecture
Below diagram highlights the architecture of the centralized logging for the additional log sources mentioned above. Centralized logging architecture has hub and spoke setup. Hub part of the setup is encapsulated in a Terraform baseline logging module and this module is invoked while setting up sandbox or prod environment through css-aws-account. Baseline logging module outputs the names of the S3 bucket setup as destination for the central logging. These bucket names are accessible to the vended AWS accounts through terraform_remote_source data source.

PL Centralized Logging Architecture - Overall Architecture v0.2.png

![image](https://user-images.githubusercontent.com/73198116/113441454-339fe000-93a3-11eb-9c35-c34885b2a986.png)


Details on the setup in Log Archive account

2nd S3 bucket is created in the Control Tower log archive account that would allow relevant AWS service principals to deliver logs to this bucket. For VPC flow logs, NLB/ALB access logs, the service principal is delivery.logs.amazonaws.com'; for GuardDuty service principal is 'guardduty.amazonaws.com';
Access logging for the 2nd log aggregation S3 bucket is enabled and it delivers the logs to the S3 bucket that is created by Control Tower to store S3 access logs
GuardDuty findings should be encrypted before delivery to the central log S3 bucket. GuardDuty findings from both primary & secondary regions can be exported to central log S3 bucket. NOTE: since only GuardDuty, Config, CloudTrail and S3 access logs are ingested to SIEM (Splunk), a separate S3 bucket is created as source to SIEM tools
VPC flow logs from both primary & secondary regions can be exported to central log S3 bucket
NLB and ALB access logs require to have S3 bucket in the same region. So an S3 bucket is created in secondary region (us-east-1) and cross-region replication is enabled between bucket in secondary region with the central log S3 bucket created above. NLB & ALB access logs delivery has specific bucket policy requirements as highlighted here - https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html  & https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-logging-bucket-permissions 
CloudWatch logs: There are four recommended methods for retrieving log data from CloudWatch Logs:
** Use subscription filters to stream log data to another receiving source in real time
** Run a query with CloudWatch Logs Insights
** Export log data to Amazon S3 for batch use cases - Log data can take up to twelve hours to become available for export from CloudWatch Logs. For real-time analysis and processing, use subscription filters. More details on export log data to S3 is given here - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html 
** Call GetLogEvents or FilterLogEvents in the CloudWatch API
As PL is planning to use CloudWatch logs for the real time monitoring, subscription filters are setup for the CloudWatch log groups in the Control Tower managed accounts to stream log to central S3 bucket in log archive account.

PL Centralized Logging (Overall Architecture) (11)-CW KDF Centralized Logging (4).png

In the AWS Control Tower Log archive account, an Amazon CloudWatch Log Destination aggregates logs and facilitates streaming logs with Amazon Kinesis Data Firehose (KDF) ultimately to the Amazon S3 centralized log bucket. Kinesis Data Firehose and Log destinations are created for both primary & secondary region. A new customer managed customer master key (CMK) is used with Amazon Kinesis Data Firehose, to encrypt objects in Amazon S3.
As part of the TFE pipeline, a Lambda function is invoked to update Log Destination policy (in both primary & secondary region), by adding permissions for newly vended AWS Control Tower managed account.
Also, in AWS Control Tower managed accounts, AWS CloudWatch Log Subscriptions are setup for existing log groups. An Amazon CloudWatch Event rule is setup so that it can trigger an AWS Lambda function to setup Log Subscriptions for new log groups as they are created.
Protecting the Log Infrastructure
Preventive Control
SCP protecting log configuration changing service actions
Prevent users from deleting VPC flow logs
Prevent users from disabling CloudWatch or altering its configuration
Prevent users from modifying GuardDuty configuration
Detective Control - Monitor Log Configuration Posture
Guardrails for log archive bucket
Log retention period
Ensure CloudTrail log file validation is enabled (utilize Control Tower mandatory guardrail)
Logs bucket encrypted at rest (utilize Control Tower mandatory guardrail)
Bucket storing logs are not publicly accessible (utilize Control Tower mandatory guardrail)
Guardrails deployed in each Control Tower managed AWS account to protect log configuration
Deploy AWS Config rule or Dome9 monitoring rule - to make sure the different log destinations are set to the Org level log aggregation S3 bucket
Making sure CloudWatch log destination set to centralized log destination
How to use Centralized Logging?
Centralized logging baseline in an environment (i.e., sbx/prod) makes the centralized logging S3 bucket names accessible through teeraform_remote_state data.

data "terraform_remote_state" "logging" {
  backend = "remote"
  config = {
    organization = "PacificLife"
    hostname     = "terraform-dev.plcssdev.com"
    workspaces = {
      name = "pl-logging-sandbox-css"
    }
  }
}
NOTE: Make sure appropriate workspace name is used.

VPC Flow Logs
From the above defined terraform_remote_state data, you set VPC flow log destination S3 bucket name as follows.

module "vpc-west" {
    source = "./us-west-2"
    logging_bucket = data.terraform_remote_state.logging.outputs.primary_bucket_name
    providers = {
        aws = aws.usw2
  }
}
NOTE: Vended AWS account requiring local copy of VPC flow logs can create additional flow log and specify their choice of destination

NLB/ALB Access logs
From the above defined terraform_remote_state data, you can set ALB/NLB access logs as follows.

Note:

Since ALB/NLB access log S3 destination has to be in the same region, use primary bucket name in primary region and secondary bucket name in secondary region. i.e., for the ALB/NLB access logs in the primary region, specify the central log S3 bucket as destination and for the ALB/NLB access logs in secondary region, specify the log S3 bucket in the secondary region.
module "alb-east" {
    source = "./us-east-1"
    logging_bucket = data.terraform_remote_state.logging.outputs.primary_bucket_name
    logging_bucket_secondary =  data.terraform_remote_state.logging.outputs.secondary_bucket_name
    providers = {
        aws = aws.use1
  }
}

CloudWatch Logs
Sending of the application logs to CloudWatch logs and then to the centralized logging bucket is enabled as part of the account common baseline and by invoking common-baseline TF module.

module "common-baseline" {
  source                        = "terraform-dev.plcssdev.com/PacificLife/ctfx-baseline-common/aws"
  version                       = "0.0.6"
  logging_account               = data.terraform_remote_state.logging.outputs.logging_account
  providers = {
    aws = aws.usw2
    aws.secondary = aws.use1
  }
}
NOTE: setting variable enable_logging_baseline to false while invoking common-baseline TF module, disables centralized logging for CloudWatch logs.

GuardDuty Findings
GuardDuty enabled through Organization and GuardDuty findings are aggregated in the audit account. Aggregated findings are then exported to the central log S3 bucket. TF extension takes care of automatically setting up the GuardDuty in vended AWS accounts

SIEM Integration
Splunk Integration
Currently only the Config, CloudTrail, GuardDuty findings and S3 access logs are ingested to Splunk. S3 event notification is setup for the S3 buckets storing these logs so that when new objects are created, notifications are sent on Amazon SQS queue. Splunk uses SQS-based S3 input method to ingest the log data.
